{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "bbe159d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "\n",
    "import tetris\n",
    "importlib.reload(tetris);\n",
    "from tetris import Tetris\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "1aeda2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiance:\n",
    "\n",
    "    def __init__(self, state, new_state, action, reward, n_actions):\n",
    "        self.state = state\n",
    "        self.new_state = new_state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.done = None\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Experiance ( {self.state}, {self.new_state}, {self.action}, {self.reward}, {n_actions}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "db87e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.memory = []\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.current_index = 0\n",
    "\n",
    "    def push(self, exp):\n",
    "        if self.current_index == self.capacity:\n",
    "            self.current_index = 0\n",
    "\n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory[self.current_index] = exp\n",
    "        else:\n",
    "            self.memory.append(exp)\n",
    "        self.current_index += 1\n",
    "        \n",
    "    def sample(self):\n",
    "        return [self.memory[i] for i in np.random.permutation(len(self.memory))[:self.batch_size]]\n",
    "\n",
    "    def can_provide_sample(self):\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def stack_memory(memory_list):\n",
    "        actions = []\n",
    "        states = []\n",
    "        new_states = []\n",
    "        rewards = []\n",
    "        n_actions = []\n",
    "        \n",
    "        for exp in memory_list:\n",
    "            states.append(exp.state)\n",
    "            new_states.append(exp.new_state)\n",
    "            actions.append(exp.action)\n",
    "            rewards.append(exp.reward)\n",
    "            n_actions.append(exp.n_actions)\n",
    "            \n",
    "        return (torch.stack(states), torch.stack(new_states), \n",
    "                torch.tensor(actions), torch.tensor(rewards), \n",
    "                torch.tensor(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "e4c3fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, width, height):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=width*height, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.out = nn.Linear(in_features=64, out_features=9*4)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # expected t shape (batch_size, width, height)\n",
    "        t = t.reshape(t.shape[0], -1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "2ce1e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(init=True, repr=True)\n",
    "class Agent:\n",
    "    \n",
    "    lr : float\n",
    "    discount_factor : float\n",
    "    ep : float\n",
    "    ep_decay : float\n",
    "    ep_min : float\n",
    "\n",
    "#     def __post_init__(self):\n",
    "\n",
    "    def take_action(self, state, possible_actions):\n",
    "        self.ep = max(self.ep_min, self.ep * (1-self.ep_decay))\n",
    "        if self.ep > random.random():\n",
    "            # explore\n",
    "            return np.random.randint(len(possible_actions))\n",
    "        else:\n",
    "            # exploit\n",
    "            action_pred = QValueEstimator.policy_predict(state.unsqueeze(0)).argmax().item()\n",
    "            if action_pred < len(possible_actions):\n",
    "                return action_pred\n",
    "            return np.random.randint(len(possible_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "2f66bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValueEstimator:\n",
    "    \n",
    "    policy_network = None\n",
    "    target_network = None\n",
    "    optimizer = None\n",
    "    \n",
    "    def __init__(self, width, height):\n",
    "        QValueEstimator.policy_network = DQN(width, height)\n",
    "        QValueEstimator.target_network = DQN(width, height)\n",
    "        QValueEstimator.target_network.eval()\n",
    "        \n",
    "        QValueEstimator.update_target_network()\n",
    "        \n",
    "        QValueEstimator.optimizer = torch.optim.Adam(\n",
    "            QValueEstimator.policy_network.parameters(),\n",
    "            lr=0.01\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def update_target_network():\n",
    "        QValueEstimator.target_network.load_state_dict(QValueEstimator.policy_network.state_dict())\n",
    "        \n",
    "    @staticmethod\n",
    "    def policy_predict(t):\n",
    "        return QValueEstimator.policy_network(t)\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_policy_network(agent, memory_exps):\n",
    "        states, new_states, actions, rewards, n_actions = ReplayMemory.stack_memory(memory_exps)\n",
    "        next_state_pred = QValueEstimator.target_network(new_states).max(dim=1)[0]\n",
    "        next_state_pred[n_actions] = 0\n",
    "        optimal_actions = torch.tensor(rewards + agent.discount_factor * next_state_pred, requires_grad=True)\n",
    "        pred_actions = torch.tensor([p[a] for a, p in zip(actions, QValueEstimator.policy_network(states))], requires_grad=True)\n",
    "#         print(pred_actions, optimal_actions)\n",
    "        QValueEstimator.optimizer.zero_grad()\n",
    "#         loss = torch.sum((optimal_actions - pred_actions)**2)\n",
    "        loss = F.mse_loss(pred_actions, optimal_actions)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        QValueEstimator.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "108b2fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisEnvManager:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tetris = Tetris()\n",
    "        self.tetris.reset()\n",
    "        \n",
    "        self.action_space_n = 9 * 4\n",
    "        \n",
    "    def reset(self):\n",
    "        self.tetris.reset()\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.tetris.step(action, render=False)\n",
    "    \n",
    "    def get_possible_actions(self):\n",
    "        return self.tetris.get_next_states().keys()\n",
    "        \n",
    "    def get_current_state(self):\n",
    "        numpy_inp = np.array(self.tetris.get_current_board_state()).astype(np.float32).flatten()\n",
    "        numpy_inp[numpy_inp != 0] = 1\n",
    "        input_tensor = torch.tensor(numpy_inp, requires_grad=True)\n",
    "        return input_tensor.reshape(self.height, self.width)\n",
    "        \n",
    "    def render(self):\n",
    "        s = \"\"\n",
    "        for row in self.tetris.get_current_board_state():\n",
    "            for char in row:\n",
    "                if char == 0:\n",
    "                    s += \" .\"\n",
    "                else:\n",
    "                    s += \" o\"\n",
    "            s += \"\\n\"\n",
    "        print(s)\n",
    "        \n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.tetris.width\n",
    "    \n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.tetris.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "6f4df982",
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris = Tetris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "97e4ef65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetris.width, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "fded042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris_env = TetrisEnvManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "e545b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, done = tetris_env.step((0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "7e5ab0c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2v/0tr3wk2s1tn4fntw0fbqt3fh0000gn/T/ipykernel_26554/96551673.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQValueEstimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtetris_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/2v/0tr3wk2s1tn4fntw0fbqt3fh0000gn/T/ipykernel_26554/2186064831.py\u001b[0m in \u001b[0;36mpolicy_predict\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mQValueEstimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "QValueEstimator.policy_predict(tetris_env.get_current_state().unsqueeze(0)).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "40d06699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . . . . o o . . . .\n",
      " . . . . . o o . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . . . . . . . . . .\n",
      " . o . . . . . . . .\n",
      " o o o . . . . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tetris_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d72c4",
   "metadata": {},
   "source": [
    "**Train loop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "f1382101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Ep 14***********\n",
      "Action (6, 0)\n",
      "Total reward 10\n",
      "Ep 0.7740428188605086\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2v/0tr3wk2s1tn4fntw0fbqt3fh0000gn/T/ipykernel_26554/582247937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_provide_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mQValueEstimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2v/0tr3wk2s1tn4fntw0fbqt3fh0000gn/T/ipykernel_26554/2186064831.py\u001b[0m in \u001b[0;36mupdate_policy_network\u001b[0;34m(agent, memory_exps)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_policy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_exps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_exps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mnext_state_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValueEstimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnext_state_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "n_episodes = 10000\n",
    "done = False\n",
    "target_net_update = 30\n",
    "\n",
    "agent = Agent(\n",
    "    lr=0.001,\n",
    "    discount_factor=0.99,\n",
    "    ep=1,\n",
    "    ep_decay=0.001,\n",
    "    ep_min=0.2\n",
    ")\n",
    "\n",
    "QValueEstimator(tetris_env.width, tetris_env.height)\n",
    "\n",
    "replay_memory = ReplayMemory(10000, 256)\n",
    "\n",
    "all_rewards = []\n",
    "\n",
    "for n in range(1, n_episodes+1):\n",
    "    \n",
    "    tetris_env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    current_state = tetris_env.get_current_state()\n",
    "    \n",
    "    step = 1\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        possilble_actions = list(tetris_env.get_possible_actions())\n",
    "        \n",
    "        action = agent.take_action(current_state, possilble_actions)\n",
    "        \n",
    "        reward, done = tetris_env.step(possilble_actions[action])\n",
    "        \n",
    "        next_state = tetris_env.get_current_state()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "#         tetris_env.render()\n",
    "#         plt.plot(all_rewards)\n",
    "#         display.display(plt.show())\n",
    "        print(f\"***********Ep {n}***********\")\n",
    "        print(\"Action\", possilble_actions[action])\n",
    "        total_reward += reward\n",
    "        print(\"Total reward\", total_reward)\n",
    "        print(\"Ep\", agent.ep)\n",
    "        \n",
    "#         time.sleep(1)\n",
    "        \n",
    "        replay_memory.push(Experiance(current_state, next_state, action, reward, len(possilble_actions)))\n",
    "        \n",
    "        current_state = next_state.detach().clone()\n",
    "        \n",
    "        if replay_memory.can_provide_sample():\n",
    "            \n",
    "            QValueEstimator.update_policy_network(agent, replay_memory.sample())\n",
    "        \n",
    "        if done:\n",
    "            all_rewards.append(total_reward)\n",
    "            break\n",
    "        \n",
    "#         time.sleep(1)\n",
    "        step += 1\n",
    "    \n",
    "    if n % target_net_update == 0:\n",
    "        QValueEstimator.update_target_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090010d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "states, new_states, actions, rewards = ReplayMemory.stack_memory(replay_memory.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "43ba71a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([74790, 20, 10]),\n",
       " torch.Size([74790, 20, 10]),\n",
       " torch.Size([74790]),\n",
       " torch.Size([74790]))"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape, new_states.shape, actions.shape, rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "3cfc9328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 11])"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "dc355590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777b198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
