{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = load_digits()\n",
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.]), (64,))"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.images[0], cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    \"\"\"Logistic regression for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, l, w_shape=2):\n",
    "        rnd = np.random.RandomState(123)\n",
    "        self.l = l\n",
    "        \n",
    "        self.W = rnd.normal(0, 1, w_shape)\n",
    "        self.w0 = rnd.normal(0, 1, 1)\n",
    "        \n",
    "        # two lists for keeping track of \n",
    "        # all updated weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        self.errs = []\n",
    "        \n",
    "        self.steps_count = 0\n",
    "\n",
    "    def train(self, X, T, max_steps=3, n_similar_loss=20, similar_loss_value=0.2):\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.n_similar_loss = n_similar_loss\n",
    "        self.similar_loss_value = similar_loss_value\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            self.steps_count += 1\n",
    "            if self.__iter_over_data(X, T, n_similar_loss, similar_loss_value): \n",
    "                break\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.__sigmoid(self.W.T.dot(x) + self.w0).round()[0]\n",
    "        \n",
    "    def predict_all(self, y_train):\n",
    "        return np.array([self.predict(y) for y in y_train])\n",
    "    \n",
    "    def score(self, y_train, y_test):\n",
    "        return np.sum((self.predict_all(y_train) == y_test)) / len(y_train)\n",
    "    \n",
    "    def __iter_over_data(self, X, T, n_similar_loss, similar_loss_value):\n",
    "        for x, t in zip(X, T):\n",
    "            pred = self.__sigmoid(self.W.T.dot(x) + self.w0)[0]\n",
    "            loss = self.__loss(pred, t)\n",
    "            \n",
    "            if len(self.errs) >= n_similar_loss:\n",
    "                past_loss_dif = np.array(self.errs[-n_similar_loss:]) - loss\n",
    "                if(np.all(past_loss_dif >= 0) \n",
    "                    and \n",
    "                   np.all(past_loss_dif < similar_loss_value)):\n",
    "                    return True\n",
    "            \n",
    "            self.errs.append(loss)\n",
    "            self.__grad(x, pred, t)\n",
    "    \n",
    "    def __loss(self, y, t):\n",
    "        return -(t * np.log(y) + (1 - t) * np.log(1 - y))\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        sigm_x = 1 / (1 + np.exp(-x))\n",
    "        rnd_num = np.random.uniform(000.1, 0.1)\n",
    "        if sigm_x == 1.0:\n",
    "            return sigm_x - rnd_num\n",
    "        elif sigm_x == 0.0:\n",
    "            return sigm_x + rnd_num\n",
    "        else:\n",
    "            return sigm_x\n",
    "    \n",
    "    def __grad(self, x, predict, target):\n",
    "        self.weights.append(self.W.copy())\n",
    "        self.biases.append(self.w0.copy())\n",
    "        self.w0 -= self.l * (predict - target)\n",
    "        self.W -= self.l * (predict - target) * x\n",
    "        \n",
    "    def hyperparams(self):\n",
    "        return dict(l=self.l,\n",
    "                max_steps=self.max_steps,\n",
    "                n_similar_loss=self.n_similar_loss,\n",
    "                similar_loss_value=self.similar_loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_combinations(param_grid):\n",
    "    \n",
    "    param_names = [key for params in param_grid for (key, _) in params.items()]\n",
    "    param_values = [value for params in param_grid for (_, value) in params.items()]\n",
    "\n",
    "    values = list(product(*param_values))\n",
    "    params = [param_names for _ in range(len(values))]\n",
    "\n",
    "    possible_params = []\n",
    "\n",
    "    for param in zip(params, values):\n",
    "        param_dict = {}\n",
    "        for name, value in zip(*param):\n",
    "            param_dict[name] = value\n",
    "        possible_params.append(param_dict)\n",
    "    return possible_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(model, X, y, hyper_param_list, folds=5):\n",
    "    models_and_scores = []\n",
    "    fold_size = len(X) // folds\n",
    "    \n",
    "    # cross validation\n",
    "    for fold in range(folds):\n",
    "        mask = np.ones(len(X), dtype=bool)\n",
    "        mask[fold_size * fold: fold_size * (fold + 1)] = False\n",
    "        \n",
    "        X_train_, y_train_, X_test_, y_test_ = X[mask], X[~mask], y[mask], y[~mask]\n",
    "        \n",
    "        models = []\n",
    "        for i in range(len(hyper_param_list)):\n",
    "            mode = deepcopy(model)\n",
    "            mode.train(X_train_, X_test_, **hyper_param_list[i])\n",
    "            models.append(mode)\n",
    "        \n",
    "        models_and_scores.extend([\n",
    "            (model, model.score(y_train_, y_test_))\n",
    "            for model in models\n",
    "        ])\n",
    "    best_model_index = np.argmax(np.array(models_and_scores)[:, 1])\n",
    "    return models_and_scores[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_numbers(dig1, dig2):\n",
    "    c0 = X[np.where(y == dig1)]\n",
    "    c1 = X[np.where(y == dig2)]\n",
    "\n",
    "    l0 = np.zeros(len(y[np.where(y == dig1)]))\n",
    "    l1 = np.ones(len(y[np.where(y == dig2)]))\n",
    "    \n",
    "    data = np.concatenate((c0, c1))\n",
    "    labels = np.concatenate((l0, l1))\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "data, labels = choose_numbers(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(\n",
    "                                            data, \n",
    "                                            labels, \n",
    "                                            test_size=0.3, \n",
    "                                            random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_similar_loss': 3, 'similar_loss_value': 0.2},\n",
       " {'n_similar_loss': 3, 'similar_loss_value': 0.4},\n",
       " {'n_similar_loss': 3, 'similar_loss_value': 0.6},\n",
       " {'n_similar_loss': 3, 'similar_loss_value': 0.9},\n",
       " {'n_similar_loss': 3, 'similar_loss_value': 1.3}]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{'n_similar_loss': [3, 5, 15, 20, 30], 'similar_loss_value': [0.2, 0.4, 0.6, 0.9, 1.3]}]\n",
    "\n",
    "hyper_params = get_possible_combinations(param_grid)\n",
    "hyper_params[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.LogisticRegression at 0x7f40acb4b080>, 1.0)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model, best_score = find_best_model(\n",
    "                            LogisticRegression(l=0.2, w_shape=X_train.shape[1]), \n",
    "                            X_train, \n",
    "                            X_test, \n",
    "                            hyper_params)\n",
    "best_model, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 0.2, 'max_steps': 3, 'n_similar_loss': 3, 'similar_loss_value': 0.2}"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best hyperparams\n",
    "best_model.hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model accuracy score on test data: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Best model accuracy score on test data:', best_model.score(y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred : 0.0, Target : 0.0\n",
      "Pred : 1.0, Target : 1.0\n",
      "Pred : 1.0, Target : 1.0\n",
      "Pred : 1.0, Target : 1.0\n",
      "Pred : 0.0, Target : 0.0\n",
      "Pred : 1.0, Target : 1.0\n",
      "Pred : 0.0, Target : 0.0\n",
      "Pred : 1.0, Target : 1.0\n",
      "Pred : 0.0, Target : 0.0\n",
      "Pred : 1.0, Target : 1.0\n"
     ]
    }
   ],
   "source": [
    "# quick test\n",
    "random_indexes = np.random.randint(len(y_train), size=10)\n",
    "for y, t in zip(np.array(y_train)[random_indexes], np.array(y_test)[random_indexes]):\n",
    "    pred = best_model.predict(y)\n",
    "    print(f'Pred : {pred}, Target : {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for other numbers\n",
    "\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "data, labels = choose_numbers(4, 9)\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(\n",
    "                                            data, \n",
    "                                            labels, \n",
    "                                            test_size=0.3, \n",
    "                                            random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.LogisticRegression at 0x7f40acb15a20>, 1.0)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{'n_similar_loss': [3, 5, 15, 20, 30], 'similar_loss_value': [0.2, 0.4, 0.6, 0.9, 1.3]}]\n",
    "\n",
    "hyper_params = get_possible_combinations(param_grid)\n",
    "\n",
    "best_model, best_score = find_best_model(\n",
    "                            LogisticRegression(l=0.2, w_shape=X_train.shape[1]), \n",
    "                            X_train, \n",
    "                            X_test, \n",
    "                            hyper_params)\n",
    "\n",
    "best_model, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 0.2, 'max_steps': 3, 'n_similar_loss': 30, 'similar_loss_value': 0.2}"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model accuracy score on test data: 0.981651376146789\n"
     ]
    }
   ],
   "source": [
    "print('Best model accuracy score on test data:', best_model.score(y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
